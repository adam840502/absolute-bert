{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359223f1-d317-43a1-be68-4b773357ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9868a467-07ba-4294-98a9-0494580542db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessor import Stopwords_preprocessor\n",
    "from utils.markdown import beir_metrics_to_markdown_table\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# from rank_bm25 import BM25Okapi as BM25\n",
    "from transformers import logging, AutoTokenizer\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5b9f58-6074-4391-a283-1c57c4a2c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cace617d-0c06-41a1-a705-d5e118a859de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d3331f-6e34-4319-b419-1049b7525e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from beir import util\n",
    "# dataset =  'trec-covid' # \"nfcorpus\" \n",
    "# url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "# data_path = util.download_and_unzip(url, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecedc775-7d5d-46d8-ada5-51dc9f93fdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0593de5fb5c94e079a1df81cdbd1bb8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_name = 'scifact'\n",
    "corpus_name = 'trec-covid'\n",
    "# corpus_name = 'nfcorpus'\n",
    "\n",
    "corpus, queries, qrels = GenericDataLoader(f'data/{corpus_name}').load(split=\"test\")\n",
    "corpus_text = [v['text'] for k,v in corpus.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74415317-b92a-41ba-857d-a6fc32f1e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    return tokenizer.convert_ids_to_tokens(tokenizer.encode(x, add_special_tokens=False))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, vocabulary=tokenizer.vocab)\n",
    "# %time vectorizer.fit(corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f24ae7b-219e-4628-b1d1-1b7b8b0a1f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# text_sample = corpus[list(corpus.keys())[0]]['text']\n",
    "# res = mean_rotary_discrepency(text_sample)\n",
    "# res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8afab6a5-e82d-4247-824b-06d117a37f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vector(text):\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(ids) == 0:\n",
    "        return np.zeros(word_reprs.shape[1])\n",
    "    return word_reprs[ids].mean(axis=0)\n",
    "\n",
    "def sum_vector(text):\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return word_reprs[ids].sum(axis=0)\n",
    "\n",
    "\n",
    "def idf_mean_vector(text):\n",
    "  ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "  # return (vectorizer.idf_[ids] @ word_reprs[ids]) / (len(ids) + 1e-8) # 這個比較慢，可能跟 contiguous 有關\n",
    "  return np.einsum('ld,l', word_reprs[ids], vectorizer.idf_[ids]) / (len(ids) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae78eab6-213d-42cb-8c83-bdb7656e4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = pathlib.Path('data/limanet/')\n",
    "\n",
    "subpath = '20240422.11:52:35-batch_size_1024' #trec-covid best, batch 10000\n",
    "# subpath = '20240422.02:12:51-batch_size_1024' #for report\n",
    "subpath = '20240423.15:38:14-batch_size_1024' #rotator_lima3, dim 96, head 12, depth 3, rotary_denom 2\n",
    "\n",
    "# subpath = '20240430.16:16:30-batch_size_256' # rotator_lima4_hippo, mse+multiplet, dim128, hdim32, head32, dep3\n",
    "subpath = '20240501.12:18:59-batch_size_512' # rotator_lima4_multihippo, mse+multiplet, dim128, hdim32, head32, dep3\n",
    "# subpath = '20240502.19:17:09-batch_size_512' # rotator_lima4_noClocks, cos_dist+multiplet, dim128, hdim32, head32, dep3\n",
    "# subpath = '20240502.21:10:49-batch_size_64' # rotator_lima4_noClocks, msmarco, mse+multiplet, dim128, hdim32, head32, dep3\n",
    "# subpath = '20240502.22:32:46-batch_size_512' # rotator_lima4_noTime, mse+multiplet, dim128, hdim32, head32, dep3\n",
    "# subpath = '20240502.23:36:34-batch_size_512' # rotator_lima4_noTime, cos_dist+multiplet\n",
    "subpath = '20240504.18:44:29' # rotator_lima5\n",
    "subpath = '20240505.17:47:20'\n",
    "subpath = '20240504.05:01:22'\n",
    "\n",
    "batch_num = 70000\n",
    "\n",
    "model = torch.load(folder_path/subpath/f'batch_{batch_num}-model.pt', map_location='cpu')\n",
    "word_reprs_complex = model.predictor.all_word_embeddings()\n",
    "word_reprs = torch.concat([word_reprs_complex.real, word_reprs_complex.imag], dim=-1).detach().numpy()\n",
    "word_reprs_complex = word_reprs_complex.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fb41a6d-5548-4120-9cc6-5c22eed1c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\"\"\n",
    "# {model.dim=}\\n\\\n",
    "# {model.hidden_dim=}\\n\\\n",
    "# {model.hippo_dim=}\\n\\\n",
    "# {model.num_hippo_heads=}\\n\\\n",
    "# {model.history_dim=}\\n\\\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aec96e1-6362-47df-a6e8-7bba46b04a41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 16s, sys: 937 ms, total: 2min 17s\n",
      "Wall time: 2min 17s\n",
      "CPU times: user 8.21 ms, sys: 15 µs, total: 8.22 ms\n",
      "Wall time: 8.17 ms\n"
     ]
    }
   ],
   "source": [
    "method = idf_mean_vector\n",
    "method = mean_vector\n",
    "# method = sum_vector\n",
    "\n",
    "part = 'text'\n",
    "# part = 'title'\n",
    "\n",
    "%time text_vec_dict = OrderedDict({k: method(v[part]) for k, v in corpus.items()})\n",
    "%time query_vec_dict = OrderedDict({k: method(v) for k, v in queries.items()})\n",
    "text_vecs = np.stack(list(text_vec_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f10e4e1b-5b47-4be7-8231-a18caf29ca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 3min 39s, total: 5min\n",
      "Wall time: 8.22 s\n",
      "NDCG@1\tNDCG@3\tNDCG@5\tNDCG@10\tNDCG@100\tNDCG@1000\tMAP@1\tMAP@3\tMAP@5\tMAP@10\tMAP@100\tMAP@1000\tRecall@1\tRecall@3\tRecall@5\tRecall@10\tRecall@100\tRecall@1000\tP@1\tP@3\tP@5\tP@10\tP@100\tP@1000\n",
      "0.23\t0.22693\t0.24486\t0.22074\t0.16276\t0.12917\t0.00065\t0.00153\t0.00244\t0.00377\t0.01756\t0.03715\t0.00065\t0.00174\t0.00326\t0.00548\t0.0332\t0.12164\t0.26\t0.25333\t0.28\t0.24\t0.1692\t0.0642\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "||NDCG|MAP|Recall|P|\n",
       "|-|-|-|-|-|\n",
       "|@1|0.2300|0.0006|0.0006|0.2600|\n",
       "|@3|0.2269|0.0015|0.0017|0.2533|\n",
       "|@5|0.2449|0.0024|0.0033|0.2800|\n",
       "|@10|0.2207|0.0038|0.0055|0.2400|\n",
       "|@100|0.1628|0.0176|0.0332|0.1692|\n",
       "|@1000|0.1292|0.0372|0.1216|0.0642|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = 'euclidean'\n",
    "# metric = 'cosine'\n",
    "\n",
    "def score(query_vector, metric=metric):\n",
    "    return (1/pairwise_distances(query_vector[None, :], text_vecs, metric=metric))[0]\n",
    "\n",
    "\n",
    "%time results = {qid: dict(zip(text_vec_dict.keys(), score(query_vector).tolist())) \\\n",
    "            for qid, query_vector in query_vec_dict.items()}\n",
    "\n",
    "metrics = EvaluateRetrieval.evaluate(qrels, results, [1, 3, 5, 10, 100, 1000])\n",
    "\n",
    "flatten_metrics = {k: v for metric_type in metrics for k, v in metric_type.items()}\n",
    "metric_names, metric_values = zip(*flatten_metrics.items())\n",
    "print(*metric_names, sep='\\t')\n",
    "print(*metric_values, sep='\\t')\n",
    "print()\n",
    "\n",
    "md = beir_metrics_to_markdown_table(*metrics)\n",
    "Markdown(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accbb11c-f59c-4cef-9330-d0fbc4c84014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(model.limas)):\n",
    "#     lima_shape = model.limas[i].lima_shape\n",
    "#     print(lima_shape)\n",
    "#     print(f'{i}: {lima_shape.min()}, {lima_shape.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcf9539e-9188-412d-ab27-e3b763375132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(0.5338, requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predictor.rotary_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdecec59-c1d6-4131-b37e-e78e0b3bdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write first 10 questions and top 10 answer to file\n",
    "\n",
    "# samples = list(results.items())[:10]\n",
    "# for q_num, score_dict in samples:\n",
    "#     with open(f'question_{q_num}.txt', 'w') as f:\n",
    "#         f.write(f'{queries[q_num]}\\n\\n')\n",
    "#         tokens = tokenizer.convert_ids_to_tokens(tokenizer(queries[q_num], add_special_tokens=False)['input_ids'])\n",
    "#         f.write(f'{tokens}\\n\\n')\n",
    "        \n",
    "#         text_ids, text_scores = zip(*score_dict.items())\n",
    "#         text_scores = np.array(text_scores)\n",
    "#         top_10_idx = np.argsort(text_scores)[:-10:-1]\n",
    "\n",
    "#         for idx in top_10_idx:\n",
    "#             f.write(f'{corpus[text_ids[idx]]}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f43406fa-1299-46a9-8462-4398095cc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test: 看每個字往時間方向逆向轉一個 t 後，附近的字為何。理論來說會是跟這個字無關的字 (text independent)，因為這個旋轉抵銷了時間旋轉\n",
    "\n",
    "# inverse_metric_theta = - 1/model.predictor.rotary_denom**(model.predictor.dimension_nums/model.predictor.dim)\n",
    "# inverse_pos_rotation = torch.complex(inverse_metric_theta.cos(), inverse_metric_theta.sin())\n",
    "# least_effective_position_of_the_word = model.predictor.all_word_embeddings() * inverse_pos_rotation\n",
    "# least_effective_position_of_the_word = torch.concat([least_effective_position_of_the_word.real, least_effective_position_of_the_word.imag], dim=-1).detach().numpy()\n",
    "\n",
    "# least_effective_position_of_the_word.shape\n",
    "\n",
    "# %time d = pairwise_distances(word_reprs, least_effective_position_of_the_word, metric='euclidean') # metric='cosine'\n",
    "\n",
    "# %time pair = d.argsort(axis=1)[:, :10]\n",
    "\n",
    "# for input_id in tokenizer.encode(text_sample):\n",
    "#     print(f'{tokenizer.convert_ids_to_tokens(input_id)}: {tokenizer.convert_ids_to_tokens(pair[input_id])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
