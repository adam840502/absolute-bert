{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20327,
     "status": "ok",
     "timestamp": 1712972152246,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "7qRPp5WdJF6K",
    "outputId": "a3fa7ee1-8df7-4eec-db1c-c966a9f6c823"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda:5' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712972152246,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "th6oFB4JJXbg"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# import datasets\n",
    "# from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import utils.load_corpus as load_corpus\n",
    "import utils.losses as l\n",
    "\n",
    "import utils.RNN as RNN\n",
    "import model.absolute_bert as Abs_bert\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712972152247,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "HzjhZio7KkHu"
   },
   "outputs": [],
   "source": [
    "PATH = \"data\"\n",
    "folder_path = Path(f\"limanet\")\n",
    "os.makedirs(PATH/folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1712972153018,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "ORJKy4IGa9Er",
    "outputId": "f8fe9709-4c29-410e-b62e-a9a12b380b46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2293, 4743, 1012,  102,    0,    0,    0],\n",
       "        [ 101, 7632, 1010, 1045, 1005, 1049, 3960, 1012,  102]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_type = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_type)\n",
    "text = [\"I love bird.\", \"Hi, I'm Bob.\"]\n",
    "tokenized = tokenizer(text, return_tensors='pt', padding=True)\n",
    "tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1712972170349,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "PKq3J3zpmtsr",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "af55ae0f-ad29-4859-d3d5-0dd5bee53ac6"
   },
   "outputs": [],
   "source": [
    "# def transform(batch):\n",
    "#   tokenized = tokenizer(batch['text'], return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "#   packed = torch.nn.utils.rnn.pack_padded_sequence(tokenized['input_ids'], tokenized['attention_mask'].sum(dim=1), enforce_sorted=False, batch_first=True)\n",
    "#   to_return = {\n",
    "#     'data': packed.data,\n",
    "#     'batch_sizes': packed.batch_sizes,\n",
    "#     'attention_mask': tokenized['attention_mask'],\n",
    "#     'sorted_indices': packed.sorted_indices,\n",
    "#     'unsorted_indices': packed.unsorted_indices\n",
    "#   }\n",
    "#   return to_return\n",
    "\n",
    "# dataset.set_transform(transform)\n",
    "\n",
    "# dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1712972170350,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "OksOAPJwUgdk",
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # test\n",
    "# rotator_depth = 3\n",
    "\n",
    "# import importlib\n",
    "# rotator = importlib.reload(rotator)\n",
    "# RNN = importlib.reload(RNN)\n",
    "\n",
    "# model = RNN.RNN(rotator.Rotator(tokenizer.vocab_size, depth=rotator_depth, dim=128, num_heads=32, hippo_dim=16, num_hippo_heads=8))#.to(device)\n",
    "# model.eval()\n",
    "# batch = dataset['train'][:3]\n",
    "\n",
    "# gen = model(**batch)\n",
    "# predicts, targets = [torch.concat(res) for res in zip(*gen)]\n",
    "# predicts.shape, targets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/lima/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for bookcorpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bookcorpus\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/adam/miniconda3/envs/lima/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 9617,   103,   103,  ...,  2163,   103,   103],\n",
      "        [ 1010,  2030, 23560,  ..., 11140,  2964,  1012],\n",
      "        [ 1999,   103,  2197,  ...,  2008,  2009,  2003],\n",
      "        ...,\n",
      "        [29279,   103, 15161,  ...,   103,  1996,  2451],\n",
      "        [ 1012, 14260,  1010,  ...,  2012,   103,   103],\n",
      "        [  103,   103,   103,  ...,  1010,   103,  2500]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ -100, 11140,  2964,  ...,  -100,  1010, 18814],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  1996,  -100,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ 1012, 25962, 15161,  ...,  1999,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  1015,  1516],\n",
      "        [ 1016,  2566,  1015,  ...,  -100,  1999,  -100]])}\n",
      "CPU times: user 6.3 s, sys: 2.02 s, total: 8.32 s\n",
      "Wall time: 13.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([342, 128]),\n",
       " 'token_type_ids': torch.Size([128, 512]),\n",
       " 'attention_mask': torch.Size([342, 128]),\n",
       " 'labels': torch.Size([342, 128])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 128\n",
    "masking_probability = .3\n",
    "shuffle = True\n",
    "\n",
    "import importlib\n",
    "load_corpus = importlib.reload(load_corpus)\n",
    "\n",
    "dataset = load_corpus.Load_bookcorpus(tokenizer, batch_size=batch_size, shuffle=shuffle, cache_dir='~/data1-0756727/cache/huggingface')\n",
    "# dataset = load_corpus.load_msmarco(tokenizer, batch_size=batch_size, shuffle=shuffle, cache_dir='~/data1-0756727/cache/huggingface')\n",
    "dataset = load_corpus.Load_wiki(tokenizer, batch_size=batch_size, shuffle=shuffle, cache_dir='~/data1-0756727/cache/huggingface')\n",
    "\n",
    "dataset.transformer(masking_probability=masking_probability)\n",
    "print(dataset.dataset['train'][:2])\n",
    "\n",
    "train_loader = dataset.loader()\n",
    "n = next(iter(train_loader))\n",
    "# print(tokenizer.decode(n['data']))\n",
    "{key: n[key].shape for key in n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# class Get_transform:\n",
    "\n",
    "#   def __init__(self, masking_probability=0.15, max_length=128, num_chunks=4):\n",
    "    \n",
    "#     self.max_length = max_length\n",
    "#     self.num_chunks = num_chunks\n",
    "\n",
    "#   def get_transform(self):\n",
    "#     def transform(batch):\n",
    "#       tokenized = tokenizer(batch['text'],\n",
    "#         return_tensors='pt',\n",
    "#         padding=True,\n",
    "#         add_special_tokens=False,\n",
    "#         max_length=self.max_length*self.num_chunks,\n",
    "#         truncation=True)\n",
    "#       input_ids = tokenized['input_ids'].view(-1, self.max_length)\n",
    "#       attention_mask = tokenized['attention_mask'].view(-1, self.max_length)\n",
    "#       lengths = attention_mask.sum(dim=1)\n",
    "#       indices = ~(lengths == 0)\n",
    "  \n",
    "#       input_ids, labels = self.collator.torch_mask_tokens(input_ids[indices])\n",
    "      \n",
    "#       return tokenized | {'input_ids': input_ids, 'attention_mask': attention_mask[indices], 'labels': labels}\n",
    "#     return transform\n",
    "\n",
    "# get_transform = Get_transform(masking_probability, max_length=256, num_chunks=4)\n",
    "# dataset.dataset.set_transform(get_transform.get_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sampled_softmax_cross_entropy(nn.Module):\n",
    "  \"\"\"https://douglasorr.github.io/2021-10-training-objectives/3-sampled/article.html\"\"\"\n",
    "  \n",
    "  def __init__(self, model, num_sampling=100):\n",
    "    super().__init__()\n",
    "    self.num_sampling = num_sampling\n",
    "    self.model = model\n",
    "\n",
    "  def forward(self, predictions, labels):\n",
    "    \"\"\"\n",
    "    preds: shape [batch_size, dim]\n",
    "    labels: shape [batch_size, dim]\n",
    "    \"\"\"\n",
    "    # batch_sizes = predictions.shape[:-1]\n",
    "    \n",
    "    # model = ...  # returns (batch_size x embedding_size)\n",
    "    projection = self.model.word_embeddings()  # shape (n_classes x embedding_size)\n",
    "    n_classes = projection.shape[0]\n",
    "    \n",
    "    # 2. Get target label scores, paired_inner_product(pred_emb, label_emb)\n",
    "    label_scores = (predictions * projection[labels, :]).sum(-1) + self.model.bias[labels]\n",
    "    \n",
    "    # 3. Sample shared noise & get scores\n",
    "    samples = torch.randint(high=n_classes, size=[self.num_sampling]).to(labels.device)\n",
    "    noise_scores = predictions @ projection[samples, :].T + self.model.bias[None, samples]\n",
    "    noise_scores += np.log(n_classes - 1)\n",
    "    \n",
    "    # 4. Reject samples matching target label & correct for remaining samples\n",
    "    reject_samples = (labels[..., None] == samples[None, :]) & (labels[..., None] != -100)  #後面是 collator 會把非預測目標填為 -100\n",
    "    noise_scores -= 1e6 * reject_samples\n",
    "    noise_scores -= torch.log((self.num_sampling - reject_samples.sum(-1, keepdims=True)).float())\n",
    "    \n",
    "    # 5. Apply regular softmax cross entropy\n",
    "    scores = torch.cat([label_scores[..., None], noise_scores], dim=-1)\n",
    "    pseudo_label = torch.masked_fill(labels.clone(), labels != -100, 0).view(-1)\n",
    "    loss = torch.nn.functional.cross_entropy(scores.view(-1, scores.shape[-1]), pseudo_label)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1712973346182,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "5nDVcTzpiDP9",
    "outputId": "f0885cd6-f532-45b4-b27e-1083216dcde7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 0.001\n",
       "    lr: 1e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.01 # for cross entropy\n",
    "learning_rate = 0.001 # for fine tuning\n",
    "\n",
    "model_params = {\n",
    "  'depth': 8,\n",
    "  'num_heads': 8,\n",
    "  'dim': 512,\n",
    "  # 'hidden_dim': 32,\n",
    "  # 'embedding_initialize_method': 'rand'\n",
    "}\n",
    "loss_params = {\n",
    "  'sampling_word_size': 100,\n",
    "}\n",
    "\n",
    "Abs_bert = importlib.reload(Abs_bert)\n",
    "l = importlib.reload(l)\n",
    "\n",
    "# model = RNN_base(rotator.Rotator(tokenizer.vocab_size, **model_params), weight_tying=False).to(device)\n",
    "# using_loss = l.Complex_mse()\n",
    "# using_loss = l.Complex_triplet_loss(model)\n",
    "# using_loss = l.Complex_mse_triplet_loss(model)\n",
    "# using_loss = l.Complex_mse_squared_triplet_loss(model)\n",
    "# using_loss = l.Mse_multiplet_loss(model.model, **loss_params)\n",
    "# using_loss = l.Cos_multiplet_loss(model.model, **loss_params)\n",
    "\n",
    "model = Abs_bert.Absolute_bert_for_masked_LM(tokenizer.vocab_size, **model_params).to(device)\n",
    "# using_loss = l.Sampled_softmax_cross_entropy(model, num_sampling=loss_params['sampling_word_size'])\n",
    "using_loss = Sampled_softmax_cross_entropy(model, num_sampling=loss_params['sampling_word_size'])\n",
    "\n",
    "# model = RNN_base(rotator.Rotator(tokenizer.vocab_size, **model_params)).to(device)\n",
    "# using_loss = nn.CrossEntropyLoss()\n",
    "# using_loss = l.Cross_entropy_l2_embedding(model.model, min_squared_norm=model.model.dim)\n",
    "# using_loss = l.Cross_entropy_l2_embedding(model.model, l2_squared_weight=1e-3)\n",
    "\n",
    "# subpath = '20240624.12:08:14' # 4 ssm_ce_with_bias\n",
    "# batch_num = 25000\n",
    "# model = torch.load(PATH/folder_path/subpath/f'batch_{batch_num}-model.pt', map_location=device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.LinearLR(optimizer, total_iters=10, start_factor=0.01)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test\n",
    "# batch = next(iter(train_loader))\n",
    "# batch_cuda = {k: v.to(device) for k, v in batch.items()}\n",
    "# res = model(**batch_cuda)\n",
    "\n",
    "# # using_loss = Sampled_softmax_cross_entropy(model, num_sampling=loss_params['sampling_word_size'])\n",
    "# # l_ = using_loss(res[0], res[1])\n",
    "\n",
    "# preds = res[0] @ model.word_embeddings().T\n",
    "# l_ = nn.functional.cross_entropy(preds.view(-1, tokenizer.vocab_size), res[1].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing model output\n",
    "# batch = next(iter(train_loader))\n",
    "# batch = {key: batch[key].to(device) for key in batch}\n",
    "\n",
    "# gen = model(**batch)\n",
    "# predicts, targets = [torch.concat(res) for res in zip(*gen)]\n",
    "# losses = using_loss(predicts, targets)\n",
    "# loss = sum(losses.values())\n",
    "# loss_dict = {\n",
    "#   'total': f\"{loss.item(): .6f}\",\n",
    "#   **{key: f\"{losses[key].item(): .6f}\" for key in losses},\n",
    "# }\n",
    "\n",
    "# print(loss_dict)\n",
    "# # del batch, loss\n",
    "# # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1712973348182,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "v5owcJeinrVP",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def backward_hook(module, gin, gout):\n",
    "#   print(f\"{len(gin)=}, {len(gout)=}\")\n",
    "#   print(*[f\"{i=}, {gi.shape=}, {gi.mean()=}, {gi.min()=}, {gi.max()=}\" for i, gi in enumerate(gin)], sep='\\n')\n",
    "#   print(*[f\"{i=}, {go.shape=}, {go.mean()=}, {go.min()=}, {go.max()=}\" for i, go in enumerate(gout)], sep='\\n')\n",
    "\n",
    "# model.limas[0].register_backward_hook(backward_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1712972219114,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "fjwk_J2biBnE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20240704.21:19:36'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_string = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=8))).strftime('%Y%m%d.%H:%M:%S')\n",
    "subfolder_path = Path(time_string)\n",
    "\n",
    "os.makedirs(PATH/folder_path/subfolder_path, exist_ok=True)\n",
    "\n",
    "with open(PATH/folder_path/subfolder_path/f'parameters.json', 'w') as f:\n",
    "  f.write(json.dumps({\n",
    "    'tokenizer_type': tokenizer_type,\n",
    "    'model': str(model),\n",
    "    'model_params': model_params,\n",
    "    'learning_rate': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'masking_probability': masking_probability,\n",
    "    'shuffle': shuffle,\n",
    "    'loss_type': str(using_loss),\n",
    "    'loss_params': loss_params,\n",
    "  }))  \n",
    "time_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjU3iaLjiFsN",
    "outputId": "691fe7e1-4bb0-451a-c5ad-2e5a219c135e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50459 [00:00<?, ?it/s, loss=43.076263]/tmp/ipykernel_56272/1220748506.py:38: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly(True):\n",
      "100%|██████████| 50459/50459 [7:31:15<00:00,  1.86it/s, loss=7.647962]    \n",
      "  5%|▌         | 2735/50459 [24:41<6:17:16,  2.11it/s, loss=11.534546] "
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "save_every_n_batches = 5000\n",
    "\n",
    "multiloss = 0\n",
    "\n",
    "# os.makedirs(PATH/folder_path/subfolder_path, exist_ok=True)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch_num, epoch in enumerate(range(num_epochs)):\n",
    "  bar = tqdm(train_loader)\n",
    "\n",
    "  for batch_num, batch in enumerate(bar):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch = {key: batch[key].to(device) for key in batch}\n",
    "    predicts, targets = model(**batch)\n",
    "\n",
    "    # predicts = predicts @ model.word_embeddings().T + model.bias\n",
    "    # predicts = predicts.view(-1, tokenizer.vocab_size)\n",
    "    # targets = targets.view(-1)\n",
    "    \n",
    "    if multiloss:\n",
    "      losses = using_loss(predicts, targets)\n",
    "      loss = sum(losses.values())\n",
    "    else:\n",
    "      loss = using_loss(predicts, targets)\n",
    "    \n",
    "    loss_dict = {\n",
    "      'loss': f\"{loss.item(): .6f}\",\n",
    "    }\n",
    "    if multiloss:\n",
    "      loss_dict |= {key: f\"{losses[key].item(): .6f}\" for key in losses}\n",
    "      \n",
    "    bar.set_postfix(loss_dict)\n",
    "\n",
    "    with torch.autograd.detect_anomaly(True):\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=10, norm_type=2)\n",
    "    # loss.backward()\n",
    "    # break \n",
    "    optimizer.step()\n",
    "\n",
    "    if (batch_num % 100 == 0):\n",
    "      # torch.cuda.empty_cache()\n",
    "      loss_history.append([epoch_num, batch_num, loss_dict])\n",
    "      # print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, batch {batch_num}\")\n",
    "\n",
    "    if (batch_num % 1000 == 999):\n",
    "      scheduler.step()\n",
    "\n",
    "    if (batch_num % save_every_n_batches == 0):\n",
    "      prefix = f\"epoch_{epoch_num}-batch_{batch_num}-\"\n",
    "      torch.save(model, PATH/folder_path/subfolder_path/(prefix + f'model.pt'))\n",
    "        \n",
    "      with open(PATH/folder_path/subfolder_path/f'epoch_{eopch_num}-history.json', 'w') as f:\n",
    "        f.write(json.dumps(loss_history))  \n",
    "\n",
    "      # torch.cuda.empty_cache()\n",
    "\n",
    "    if (batch_num in [500, 1000, 1500, 2000, 2500, 5000, 7500, 12500, 17500]):\n",
    "        prefix = f\"epoch_{epoch_num}-batch_{batch_num}-\"\n",
    "        torch.save(model, PATH/folder_path/subfolder_path/(prefix + 'model.pt'))\n",
    "        \n",
    "        with open(PATH/folder_path/subfolder_path/f'epoch_{epoch_num}-history.json', 'w') as f:\n",
    "          f.write(json.dumps(loss_history))  \n",
    "\n",
    "    # del batch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('I love you.', return_tensors='pt', add_special_tokens=False)\n",
    "histories = [his.expand(4, *[-1]*(len(his.shape)-1)) for his in model.model.initial_states()['h']]\n",
    "output, next_histories = model.model.forward(inputs['input_ids'][0].to(device), histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOYhgT3ZRZ8Mnsn8jR0fZTM",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
