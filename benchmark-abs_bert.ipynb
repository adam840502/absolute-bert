{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359223f1-d317-43a1-be68-4b773357ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9868a467-07ba-4294-98a9-0494580542db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessor import Stopwords_preprocessor\n",
    "from utils.markdown import beir_metrics_to_markdown_table\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# from rank_bm25 import BM25Okapi as BM25\n",
    "from transformers import logging, AutoTokenizer\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5b9f58-6074-4391-a283-1c57c4a2c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cace617d-0c06-41a1-a705-d5e118a859de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d3331f-6e34-4319-b419-1049b7525e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from beir import util\n",
    "# dataset =  'trec-covid' # \"nfcorpus\" \n",
    "# url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "# data_path = util.download_and_unzip(url, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a191d-035e-4a54-8512-ff756a1c44e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecedc775-7d5d-46d8-ada5-51dc9f93fdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87f3714e0af4650b74becde5587586e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_name = 'scifact'\n",
    "# corpus_name = 'trec-covid'\n",
    "# corpus_name = 'nfcorpus'\n",
    "\n",
    "corpus, queries, qrels = GenericDataLoader(f'data/{corpus_name}').load(split=\"test\")\n",
    "corpus_text = [v['text'] for k,v in corpus.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74415317-b92a-41ba-857d-a6fc32f1e611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/lima/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/adam/miniconda3/envs/lima/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1381: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.96 s, sys: 45 ms, total: 6 s\n",
      "Wall time: 5.98 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(tokenizer=&lt;function tokenize at 0x7f1d3d147c40&gt;,\n",
       "                vocabulary={&#x27;!&#x27;: 999, &#x27;&quot;&#x27;: 1000, &#x27;#&#x27;: 1001, &#x27;##!&#x27;: 29612,\n",
       "                            &#x27;##&quot;&#x27;: 29613, &#x27;###&#x27;: 29614, &#x27;##$&#x27;: 29615,\n",
       "                            &#x27;##%&#x27;: 29616, &#x27;##&amp;&#x27;: 29617, &quot;##&#x27;&quot;: 29618,\n",
       "                            &#x27;##(&#x27;: 29619, &#x27;##)&#x27;: 29620, &#x27;##*&#x27;: 29621,\n",
       "                            &#x27;##+&#x27;: 29622, &#x27;##,&#x27;: 29623, &#x27;##-&#x27;: 29624,\n",
       "                            &#x27;##.&#x27;: 29625, &#x27;##/&#x27;: 29626, &#x27;##0&#x27;: 2692,\n",
       "                            &#x27;##00&#x27;: 8889, &#x27;##01&#x27;: 24096, &#x27;##0s&#x27;: 16223,\n",
       "                            &#x27;##1&#x27;: 2487, &#x27;##10&#x27;: 10790, &#x27;##100&#x27;: 18613,\n",
       "                            &#x27;##11&#x27;: 14526, &#x27;##12&#x27;: 12521, &#x27;##13&#x27;: 17134,\n",
       "                            &#x27;##14&#x27;: 16932, &#x27;##15&#x27;: 16068, ...})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(tokenizer=&lt;function tokenize at 0x7f1d3d147c40&gt;,\n",
       "                vocabulary={&#x27;!&#x27;: 999, &#x27;&quot;&#x27;: 1000, &#x27;#&#x27;: 1001, &#x27;##!&#x27;: 29612,\n",
       "                            &#x27;##&quot;&#x27;: 29613, &#x27;###&#x27;: 29614, &#x27;##$&#x27;: 29615,\n",
       "                            &#x27;##%&#x27;: 29616, &#x27;##&amp;&#x27;: 29617, &quot;##&#x27;&quot;: 29618,\n",
       "                            &#x27;##(&#x27;: 29619, &#x27;##)&#x27;: 29620, &#x27;##*&#x27;: 29621,\n",
       "                            &#x27;##+&#x27;: 29622, &#x27;##,&#x27;: 29623, &#x27;##-&#x27;: 29624,\n",
       "                            &#x27;##.&#x27;: 29625, &#x27;##/&#x27;: 29626, &#x27;##0&#x27;: 2692,\n",
       "                            &#x27;##00&#x27;: 8889, &#x27;##01&#x27;: 24096, &#x27;##0s&#x27;: 16223,\n",
       "                            &#x27;##1&#x27;: 2487, &#x27;##10&#x27;: 10790, &#x27;##100&#x27;: 18613,\n",
       "                            &#x27;##11&#x27;: 14526, &#x27;##12&#x27;: 12521, &#x27;##13&#x27;: 17134,\n",
       "                            &#x27;##14&#x27;: 16932, &#x27;##15&#x27;: 16068, ...})</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(tokenizer=<function tokenize at 0x7f1d3d147c40>,\n",
       "                vocabulary={'!': 999, '\"': 1000, '#': 1001, '##!': 29612,\n",
       "                            '##\"': 29613, '###': 29614, '##$': 29615,\n",
       "                            '##%': 29616, '##&': 29617, \"##'\": 29618,\n",
       "                            '##(': 29619, '##)': 29620, '##*': 29621,\n",
       "                            '##+': 29622, '##,': 29623, '##-': 29624,\n",
       "                            '##.': 29625, '##/': 29626, '##0': 2692,\n",
       "                            '##00': 8889, '##01': 24096, '##0s': 16223,\n",
       "                            '##1': 2487, '##10': 10790, '##100': 18613,\n",
       "                            '##11': 14526, '##12': 12521, '##13': 17134,\n",
       "                            '##14': 16932, '##15': 16068, ...})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    return tokenizer.convert_ids_to_tokens(tokenizer.encode(x, add_special_tokens=False))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, vocabulary=tokenizer.vocab)\n",
    "%time vectorizer.fit(corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "fa1879f8-ca2e-4f3e-b445-ed350ebec0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Absolute_attention(nn.Module):\n",
    "  def __init__(self, dim=256, num_heads=16, hidden_dim=None, time_dim=64):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dim = dim\n",
    "    self.num_heads = num_heads\n",
    "    self.time_dim = time_dim\n",
    "\n",
    "    if isinstance(hidden_dim, int):\n",
    "      self.hidden_dim = hidden_dim\n",
    "    else:\n",
    "      assert dim % num_heads == 0\n",
    "      self.hidden_dim = dim // num_heads\n",
    "\n",
    "    # assert self.hidden_dim % 2 == 0\n",
    "    # self.time_embedding = time_embedding\n",
    "    self.time_angle = nn.Parameter(torch.rand(self.num_heads, self.time_dim))\n",
    "    self.head_time_delta = nn.Parameter(torch.rand(self.num_heads))\n",
    "\n",
    "    self.Q = nn.Linear(dim, num_heads * (self.hidden_dim + 1))\n",
    "    self.K = nn.Linear(dim, num_heads * self.hidden_dim)\n",
    "    self.V = nn.Linear(dim, num_heads * self.hidden_dim)\n",
    "    self.O = nn.Linear(num_heads * self.hidden_dim, dim)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      self.Q.bias.copy_(torch.zeros_like(self.Q.bias))\n",
    "      self.K.bias.copy_(torch.zeros_like(self.K.bias))\n",
    "      self.V.bias.copy_(torch.zeros_like(self.V.bias))\n",
    "      self.O.bias.copy_(torch.zeros_like(self.O.bias))\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "  def forward(self, tensor, attention_mask):\n",
    "    batch_length = tensor.shape[:2]\n",
    "    # print(f'{tensor.shape=}')\n",
    "    q = self.Q(tensor).view(*batch_length, self.num_heads, self.hidden_dim + 1)\n",
    "    # q = (q + attention_mask[..., None, None])\n",
    "    q = q.softmax(dim=-1)\n",
    "\n",
    "    time_angles = (torch.arange(batch_length[1]).to(self.head_time_delta.device)[:, None, None]\n",
    "                   + self.head_time_delta[None, :, None]) * self.time_angle  # shape: [length, num_heads, dim_time] ?\n",
    "    cosines, sines = time_angles.cos(), time_angles.sin()\n",
    "    time = torch.cat([cosines + sines, cosines - sines], dim=-1) / np.sqrt(self.hidden_dim)  # shape: [length, num_heads, 2*dim_time] ?\n",
    "    q = q[..., :-1, None].sum(-2) * time\n",
    "    # q shape: [batch_size, length, num_heads, 2*dim_time] = [batch_size, length, 1, 1] * [length, num_heads, 2*dim_time] ?\n",
    "\n",
    "    k = self.K(tensor).view(*batch_length, self.num_heads, self.hidden_dim)  # bthd\n",
    "    k = k.softmax(dim=-1) * attention_mask[..., None, None]\n",
    "\n",
    "    k_time_angles = torch.arange(batch_length[1]).to(self.time_angle.device)[:, None, None] * self.time_angle\n",
    "    k_cosines, k_sines = time_angles.cos(), time_angles.sin()\n",
    "    k_time = torch.cat([k_cosines + k_sines, k_cosines - k_sines], dim=-1) / np.sqrt(self.hidden_dim)\n",
    "\n",
    "    attention = torch.einsum('blhd,thd->blth', q, k_time)\n",
    "\n",
    "    v = self.V(tensor).view(*batch_length, self.num_heads, self.hidden_dim)\n",
    "    adding_comb = torch.einsum('blth,bthd->blhd', attention, v)\n",
    "\n",
    "    output = self.O(adding_comb.reshape(*batch_length, self.dim))\n",
    "\n",
    "    return self.dropout(output)\n",
    "\n",
    "class Absolute_bert(nn.Module):\n",
    "  def __init__(self, vocab_size, dim=256, num_heads=8, hidden_dim=None, depth=8, attention_type=Absolute_attention, dtype=torch.float):\n",
    "    super().__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.dim = dim\n",
    "    self.num_heads = num_heads\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.depth = depth\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, dim, _weight=torch.nn.init.xavier_normal_(torch.ones([vocab_size, self.dim])))\n",
    "    # self.embedding = nn.Embedding(vocab_size, dim, _weight=torch.rand(vocab_size, self.dim) / np.sqrt(self.dim))\n",
    "\n",
    "    self.layers = nn.ModuleList([Absolute_attention(dim=dim,\n",
    "      num_heads=num_heads,\n",
    "      hidden_dim=hidden_dim\n",
    "      ) for _ in range(depth)])\n",
    "    self.dtype = dtype\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, **kwargs):\n",
    "    # extended_attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "    # extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(self.dtype).min\n",
    "\n",
    "    tensor = self.embedding(input_ids)\n",
    "\n",
    "    for layer in self.layers:\n",
    "      output = layer(tensor, attention_mask)\n",
    "      tensor = tensor + output\n",
    "\n",
    "    return tensor\n",
    "\n",
    "class Absolute_bert_for_masked_LM(nn.Module):\n",
    "  def __init__(self, vocab_size, dim=256, num_heads=8, hidden_dim=None, depth=8, **kwargs):\n",
    "    super().__init__()\n",
    "    self.base_model = Absolute_bert(vocab_size, dim, num_heads, hidden_dim, depth, **kwargs)\n",
    "    self.bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n",
    "    tensor = self.base_model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "\n",
    "    return tensor, labels\n",
    "\n",
    "  def word_embeddings(self):\n",
    "    return self.base_model.embedding.weight\n",
    "\n",
    "subpath = '20240710.02:31:19' # sum_of_softmax\n",
    "subpath = '20240710.18:31:50' # sum_of_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e1d134f-f244-4cdc-b03c-ac156034c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Absolute_attention(nn.Module):\n",
    "  def __init__(self, dim=256, num_heads=16, hidden_dim=None, time_dim=64):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dim = dim\n",
    "    self.num_heads = num_heads\n",
    "    self.time_dim = time_dim\n",
    "\n",
    "    if isinstance(hidden_dim, int):\n",
    "      self.hidden_dim = hidden_dim\n",
    "    else:\n",
    "      assert dim % num_heads == 0\n",
    "      self.hidden_dim = dim // num_heads\n",
    "\n",
    "    # assert self.hidden_dim % 2 == 0\n",
    "    # self.time_embedding = time_embedding\n",
    "    self.time_angle = nn.Parameter(torch.rand(self.num_heads, self.time_dim))\n",
    "    self.head_time_delta = nn.Parameter(torch.rand(self.num_heads))\n",
    "\n",
    "    self.Q = nn.Linear(dim, num_heads * self.hidden_dim)\n",
    "    self.K = nn.Linear(dim, num_heads * self.hidden_dim)\n",
    "    self.V = nn.Linear(dim, num_heads * self.hidden_dim)\n",
    "    self.O = nn.Linear(num_heads * self.hidden_dim, dim)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      self.Q.bias.copy_(torch.zeros_like(self.Q.bias))\n",
    "      self.K.bias.copy_(torch.zeros_like(self.K.bias))\n",
    "      self.V.bias.copy_(torch.zeros_like(self.V.bias))\n",
    "      self.O.bias.copy_(torch.zeros_like(self.O.bias))\n",
    "\n",
    "    one = torch.zeros(self.dim)\n",
    "    one[0] = 1\n",
    "    self.one = nn.Parameter(one, requires_grad=False)\n",
    "    self.b = nn.Parameter(torch.zeros(self.num_heads))\n",
    "    \n",
    "    self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "  def forward(self, tensor, attention_mask):\n",
    "    batch_length = tensor.shape[:2]\n",
    "    # print(f'{tensor.shape=}')\n",
    "    q = self.Q(tensor).view(*batch_length, self.num_heads, self.hidden_dim)\n",
    "    # q = (q + attention_mask[..., None, None])\n",
    "    q0 = tensor @ self.one # shape: [batch_size, length]\n",
    "    q0 = q0[..., None] + self.b\n",
    "    q = torch.cat([q, q0[..., None].expand(-1, -1, -1, 1)], dim=-1)\n",
    "    q = q.softmax(dim=-1)\n",
    "    \n",
    "\n",
    "    time_angles = (torch.arange(batch_length[1]).to(self.head_time_delta.device)[:, None, None]\n",
    "                   + self.head_time_delta[None, :, None]) * self.time_angle  # shape: [length, num_heads, dim_time] ?\n",
    "    cosines, sines = time_angles.cos(), time_angles.sin()\n",
    "    time = torch.cat([cosines + sines, cosines - sines], dim=-1) / np.sqrt(self.hidden_dim)  # shape: [length, num_heads, 2*dim_time] ?\n",
    "    q = q[..., :-1, None].sum(-2) * time\n",
    "    # q shape: [batch_size, length, num_heads, 2*dim_time] = [batch_size, length, 1, 1] * [length, num_heads, 2*dim_time] ?\n",
    "\n",
    "    k = self.K(tensor).view(*batch_length, self.num_heads, self.hidden_dim)  # bthd\n",
    "    k = k.softmax(dim=-1) * attention_mask[..., None, None]\n",
    "\n",
    "    k_time_angles = torch.arange(batch_length[1]).to(self.time_angle.device)[:, None, None] * self.time_angle\n",
    "    k_cosines, k_sines = time_angles.cos(), time_angles.sin()\n",
    "    k_time = torch.cat([k_cosines + k_sines, k_cosines - k_sines], dim=-1) / np.sqrt(self.hidden_dim)\n",
    "\n",
    "    attention = torch.einsum('blhd,thd->blth', q, k_time)\n",
    "\n",
    "    v = self.V(tensor).view(*batch_length, self.num_heads, self.hidden_dim)\n",
    "    adding_comb = torch.einsum('blth,bthd->blhd', attention, v)\n",
    "\n",
    "    output = self.O(adding_comb.reshape(*batch_length, self.dim))\n",
    "\n",
    "    return self.dropout(output)\n",
    "\n",
    "class Absolute_bert(nn.Module):\n",
    "  def __init__(self, vocab_size, dim=256, num_heads=8, hidden_dim=None, depth=8, attention_type=Absolute_attention, dtype=torch.float):\n",
    "    super().__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.dim = dim\n",
    "    self.num_heads = num_heads\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.depth = depth\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, dim, _weight=torch.nn.init.xavier_normal_(torch.ones([vocab_size, self.dim])))\n",
    "    # self.embedding = nn.Embedding(vocab_size, dim, _weight=torch.rand(vocab_size, self.dim) / np.sqrt(self.dim))\n",
    "\n",
    "    self.layers = nn.ModuleList([Absolute_attention(dim=dim,\n",
    "      num_heads=num_heads,\n",
    "      hidden_dim=hidden_dim\n",
    "      ) for _ in range(depth)])\n",
    "    self.dtype = dtype\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, **kwargs):\n",
    "    # extended_attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "    # extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(self.dtype).min\n",
    "\n",
    "    tensor = self.embedding(input_ids)\n",
    "\n",
    "    for layer in self.layers:\n",
    "      output = layer(tensor, attention_mask)\n",
    "      tensor = tensor + output\n",
    "\n",
    "    return tensor\n",
    "\n",
    "class Absolute_bert_for_masked_LM(nn.Module):\n",
    "  def __init__(self, vocab_size, dim=256, num_heads=8, hidden_dim=None, depth=8, **kwargs):\n",
    "    super().__init__()\n",
    "    self.base_model = Absolute_bert(vocab_size, dim, num_heads, hidden_dim, depth, **kwargs)\n",
    "    self.bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n",
    "    tensor = self.base_model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "\n",
    "    return tensor, labels\n",
    "\n",
    "  def word_embeddings(self):\n",
    "    return self.base_model.embedding.weight\n",
    "\n",
    "# subpath = '20240710.03:33:50' # e1_as_pivot\n",
    "subpath = '20240710.15:09:00' # e1_as_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d3ca01e6-72f0-4c9e-9d6d-b0b21d359e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subpath = '20240704.20:55:54' # 4 abs_bert, bookcorpus\n",
    "subpath = '20240704.21:19:36' # 5 abs_bert, wiki\n",
    "# subpath = '20240704.21:26:11' # 6 abs_bert, bookcorpus, xavier_init_embedding\n",
    "subpath = '20240704.21:33:23' # 7 abs_bert, wiki, xavier_init_embedding\n",
    "# subpath = '20240709.10:26:55' # abs_architecture, lr 1e-5~1e-3, early exploded\n",
    "subpath = '20240709.11:13:35' # abs_architecture, lr 1e-5~1e-3, exploded at 10000\n",
    "subpath = '20240709.12:14:48' # time_attention, lr 1e-5~1e-3, exploded at before 6000\n",
    "\n",
    "subpath = '20240710.23:13:03' # all1_as_pivot, mask_prob0.15\n",
    "subpath = '20240711.03:00:33' # all1_layerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "24ce015a-4e1b-4cb8-8baa-3bd6d1382ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight norms, min: 0.15763674676418304, max: 0.35171276330947876\n",
      "min norms: ['kwan', '1661', 'winding', 'vocational', 'crushed', '1753', 'beaux', '##athan', '##fb', 'continually']\n",
      "max norms: ['well', 'bi', 'the', 'a', 'so', '.', ',', 'its', 'march', 'and']\n",
      "bias norms, min: -0.0023363677319139242, max: 0.022068168967962265\n",
      "min norms: ['[unused744]', '₃', '##ふ', '[unused279]', '⟩', '##ₘ', '[unused662]', '##জ', 'ɯ', '[unused874]']\n",
      "max norms: ['held', 'bo', '##m', '##v', '1997', 'level', 'bi', 'its', 'so', 'march']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_num = 0\n",
    "batch_num = 2000\n",
    "\n",
    "folder_path = pathlib.Path('data/limanet/')\n",
    "# model = torch.load(folder_path/subpath/f'batch_{batch_num}-model.pt', map_location='cpu')\n",
    "model = torch.load(folder_path/subpath/f'epoch_{epoch_num}-batch_{batch_num}-model.pt', map_location='cpu')\n",
    "\n",
    "word_reprs = model.word_embeddings().detach()\n",
    "\n",
    "vectors = word_reprs\n",
    "\n",
    "_, (ax_w, ax_b) = plt.subplots(1, 2, figsize=[9, 4])\n",
    "\n",
    "# weight stats\n",
    "norms = np.linalg.norm(vectors, axis=1)\n",
    "indices = norms.argsort()\n",
    "print(f\"weight norms, min: {norms.min()}, max: {norms.max()}\")\n",
    "print(\"min norms:\", tokenizer.convert_ids_to_tokens(indices[:10]))\n",
    "print(\"max norms:\", tokenizer.convert_ids_to_tokens(indices[-10:]))\n",
    "_ = ax_w.hist(norms, bins=100)\n",
    "\n",
    "# bias stats\n",
    "biases = model.bias.detach()\n",
    "indices = biases.argsort()\n",
    "print(f\"bias norms, min: {biases.min()}, max: {biases.max()}\")\n",
    "print(\"min norms:\", tokenizer.convert_ids_to_tokens(indices[:10]))\n",
    "print(\"max norms:\", tokenizer.convert_ids_to_tokens(indices[-10:]))\n",
    "if hasattr(model, 'predictor') and hasattr(model.predictor, 'base_vec'):\n",
    "  print(f\"base vec norm: {model.predictor.base_vec.norm()}\")\n",
    "_ = ax_b.hist(biases, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8afab6a5-e82d-4247-824b-06d117a37f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vector(text, word_reprs=word_reprs):\n",
    "  ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "  if len(ids) == 0:\n",
    "      return np.zeros(word_reprs.shape[1])\n",
    "  return word_reprs[ids].mean(axis=0)\n",
    "\n",
    "def sum_vector(text):\n",
    "  ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "  return word_reprs[ids].sum(axis=0)\n",
    "\n",
    "\n",
    "def idf_mean_vector(text, word_reprs=word_reprs):\n",
    "  ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "  # return (vectorizer.idf_[ids] @ word_reprs[ids]) / (len(ids) + 1e-8) # 這個比較慢，可能跟 contiguous 有關\n",
    "  return np.einsum('ld,l', word_reprs[ids], vectorizer.idf_[ids]) / (len(ids) + 1e-8)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "mask_id = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "score = cosine_similarity(word_reprs, word_reprs[[mask_id]])\n",
    "score = 1 - np.maximum(score, 0)\n",
    "scored_word_reprs = torch.from_numpy(score) * word_reprs\n",
    "\n",
    "# def cos_scored_mean_vector(text):\n",
    "#   ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "  # return scored_word_reprs[ids].sum(axis=0)\n",
    "\n",
    "cos_weighted_mean_vector = lambda text: mean_vector(text, scored_word_reprs)\n",
    "cos_weighted_idf_mean_vector = lambda text: idf_mean_vector(text, scored_word_reprs)\n",
    "\n",
    "max_norm = word_reprs[norms.argmax()]\n",
    "score = cosine_similarity(word_reprs, [max_norm])\n",
    "bipolar_reprs = torch.from_numpy(1 - np.abs(score)) * word_reprs\n",
    "\n",
    "bipolar_weighted_mean_vector = lambda text: mean_vector(text, bipolar_reprs)\n",
    "bipolar_weighted_if_mean_vector = lambda text: idf_mean_vector(text, bipolar_reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d2e4d477-966b-4a6d-8399-0d465f563a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer('I love you.', return_tensors='pt', add_special_tokens=False)\n",
    "# histories = [his.expand(4, *[-1]*(len(his.shape)-1)) for his in model.initial_states()['histories']]\n",
    "# output, next_histories = model.forward(inputs['input_ids'][0], 1, histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1aec96e1-6362-47df-a6e8-7bba46b04a41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 39s, sys: 1.23 s, total: 3min 41s\n",
      "Wall time: 6.13 s\n",
      "CPU times: user 2.27 s, sys: 0 ns, total: 2.27 s\n",
      "Wall time: 63.8 ms\n"
     ]
    }
   ],
   "source": [
    "method = idf_mean_vector\n",
    "method = mean_vector\n",
    "# method = sum_vector\n",
    "# method = cos_weighted_mean_vector\n",
    "# method = cos_weighted_idf_mean_vector\n",
    "# method = bipolar_weighted_mean_vector\n",
    "# method = bipolar_weighted_if_mean_vector\n",
    "\n",
    "part = 'text'\n",
    "# part = 'title'\n",
    "\n",
    "%time text_vec_dict = OrderedDict({k: method(v[part]) for k, v in corpus.items()})\n",
    "%time query_vec_dict = OrderedDict({k: method(v) for k, v in queries.items()})\n",
    "text_vecs = np.stack(list(text_vec_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f10e4e1b-5b47-4be7-8231-a18caf29ca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.6 s, sys: 2min 45s, total: 3min 36s\n",
      "Wall time: 3.48 s\n",
      "NDCG@1\tNDCG@3\tNDCG@5\tNDCG@10\tNDCG@100\tNDCG@1000\tMAP@1\tMAP@3\tMAP@5\tMAP@10\tMAP@100\tMAP@1000\tRecall@1\tRecall@3\tRecall@5\tRecall@10\tRecall@100\tRecall@1000\tP@1\tP@3\tP@5\tP@10\tP@100\tP@1000\n",
      "0.26667\t0.32581\t0.33893\t0.36176\t0.39684\t0.42383\t0.25278\t0.30542\t0.31306\t0.32267\t0.33043\t0.3314\t0.25278\t0.36889\t0.40083\t0.47028\t0.62778\t0.8385\t0.26667\t0.13111\t0.086\t0.05067\t0.00697\t0.00094\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "||NDCG|MAP|Recall|P|\n",
       "|-|-|-|-|-|\n",
       "|@1|0.2667|0.2528|0.2528|0.2667|\n",
       "|@3|0.3258|0.3054|0.3689|0.1311|\n",
       "|@5|0.3389|0.3131|0.4008|0.0860|\n",
       "|@10|0.3618|0.3227|0.4703|0.0507|\n",
       "|@100|0.3968|0.3304|0.6278|0.0070|\n",
       "|@1000|0.4238|0.3314|0.8385|0.0009|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = 'euclidean'\n",
    "metric = 'cosine'\n",
    "\n",
    "def score(query_vector, metric=metric):\n",
    "    return (1/pairwise_distances(query_vector[None, :], text_vecs, metric=metric))[0]\n",
    "\n",
    "%time results = {qid: dict(zip(text_vec_dict.keys(), score(query_vector).tolist())) \\\n",
    "            for qid, query_vector in query_vec_dict.items()}\n",
    "\n",
    "metrics = EvaluateRetrieval.evaluate(qrels, results, [1, 3, 5, 10, 100, 1000])\n",
    "\n",
    "flatten_metrics = {k: v for metric_type in metrics for k, v in metric_type.items()}\n",
    "metric_names, metric_values = zip(*flatten_metrics.items())\n",
    "print(*metric_names, sep='\\t')\n",
    "print(*metric_values, sep='\\t')\n",
    "print()\n",
    "\n",
    "md = beir_metrics_to_markdown_table(*metrics)\n",
    "Markdown(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2e85caf3-c5db-417f-9a62-9d56466133ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (1763804612.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[107], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    20240710.23:13:03 all1_as_pivot\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "20240711.03:00:33 # all1_layerNorm\n",
    "      0.26\t    0.33264\t0.34898\t0.37979\t0.41022\t0.43742\t0.24778\t0.31028\t0.31969\t0.33256\t0.33873\t0.33965\t0.24778\t0.38222\t0.42194\t0.51639\t0.66011\t0.87556\t0.26\t0.13556\t0.09067\t0.05567\t0.0073\t0.00098\n",
    "1000  0.27\t    0.32931\t0.35234\t0.37988\t0.40735\t0.43431\t0.25611\t0.30903\t0.32233\t0.33408\t0.33983\t0.34075\t0.25611\t0.37222\t0.4275\t0.51139\t0.63778\t0.84933\t0.27\t0.13222\t0.09133\t0.055\t0.00707\t0.00096\n",
    "1500  0.26\t    0.32501\t0.34116\t0.36399\t0.39786\t0.42506\t0.24611\t0.30319\t0.31267\t0.32229\t0.32982\t0.33072\t0.24611\t0.37222\t0.41083\t0.48028\t0.63222\t0.84683\t0.26\t0.13222\t0.088\t0.05167\t0.00703\t0.00095\n",
    "2000  0.26667\t0.32581\t0.33893\t0.36176\t0.39684\t0.42383\t0.25278\t0.30542\t0.31306\t0.32267\t0.33043\t0.3314\t0.25278\t0.36889\t0.40083\t0.47028\t0.62778\t0.8385\t0.26667\t0.13111\t0.086\t0.05067\t0.00697\t0.00094\n",
    "\n",
    "\n",
    "20240710.23:13:03 all1_as_pivot\n",
    "      0.28333  0.35116\t0.37325\t0.3929\t0.42589\t0.45214\t0.26722\t0.32875\t0.34142\t0.3503\t0.35664\t0.35756\t0.26722\t0.4\t0.45333\t0.51056\t0.66661\t0.875\t0.28333\t0.14333\t0.09667\t0.055\t0.00747\t0.00098\n",
    "2000  0.29667  0.36094\t0.38013\t0.3938\t0.42943\t0.45456\t0.28222\t0.33949\t0.35046\t0.35662\t0.36379\t0.36467\t0.28222\t0.40833\t0.45444\t0.49472\t0.66189\t0.861\t0.29667\t0.14556\t0.09733\t0.05367\t0.00747\t0.00097\n",
    "5000  0.28\t   0.34541\t0.36051\t0.38101\t0.41828\t0.44559\t0.26556\t0.32495\t0.33333\t0.34255\t0.35022\t0.35116\t0.26556\t0.39194\t0.42778\t0.48722\t0.66217\t0.87833\t0.28\t0.14\t0.092\t0.05267\t0.0074\t0.00099\n",
    "10000 0.20333  0.22944\t0.24396\t0.25555\t0.29553\t0.3273\t0.19444\t0.21963\t0.22771\t0.23288\t0.24063\t0.24184\t0.19444\t0.24806\t0.28306\t0.31556\t0.50356\t0.74733\t0.20333\t0.08889\t0.06067\t0.03467\t0.00557\t0.00086\n",
    "15000 0.13\t   0.14172\t0.161\t0.17446\t0.20445\t0.23703\t0.12278\t0.13583\t0.14638\t0.152\t0.15716\t0.15818\t0.12278\t0.15222\t0.19806\t0.23806\t0.38733\t0.64661\t0.13\t0.05444\t0.04267\t0.02567\t0.0042\t0.00074\n",
    "\n",
    "20240710.15:09:00 e1_as_pivot\n",
    "      0.29667  0.34786\t0.36542\t0.38364\t0.42196\t0.44796\t0.28444\t0.3313\t0.34157\t0.34925\t0.35645\t0.35732\t0.28444\t0.38167\t0.42444\t0.47833\t0.66167\t0.86856\t0.29667\t0.13667\t0.09133\t0.05233\t0.00737\t0.00098\n",
    "2000  0.29667  0.34141\t0.35487\t0.37834\t0.41378\t0.43959\t0.27806\t0.32333\t0.33061\t0.34079\t0.34793\t0.34881\t0.27806\t0.37722\t0.41167\t0.48083\t0.64667\t0.85039\t0.29667\t0.13444\t0.088\t0.053\t0.0072\t0.00096\n",
    "15000 0.12333  0.14826\t0.15309\t0.1676\t0.19875\t0.2339\t0.11611\t0.13847\t0.14117\t0.14728\t0.15255\t0.15365\t0.11611\t0.16694\t0.17806\t0.22306\t0.37856\t0.66011\t0.12333\t0.06\t0.03933\t0.02433\t0.00417\t0.00075\n",
    "\n",
    "20240710.18:31:50 sum_of_softmax\n",
    "15000 0.13     0.14172\t0.161\t0.17446\t0.20445\t0.23703\t0.12278\t0.13583\t0.14638\t0.152\t0.15716\t0.15818\t0.12278\t0.15222\t0.19806\t0.23806\t0.38733\t0.64661\t0.13\t0.05444\t0.04267\t0.02567\t0.0042\t0.00074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb3aba8e-e9d5-4843-b467-fbad61c6578c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Absolute_bert_for_masked_LM' object has no attribute 'initial_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI love you.\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m histories \u001b[38;5;241m=\u001b[39m [his\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m*\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mlen\u001b[39m(his\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m his \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_states\u001b[49m()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistories\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      3\u001b[0m output, next_histories \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, histories)\n",
      "File \u001b[0;32m~/miniconda3/envs/lima/lib/python3.12/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Absolute_bert_for_masked_LM' object has no attribute 'initial_states'"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('I love you.', return_tensors='pt', add_special_tokens=False)\n",
    "histories = [his.expand(4, *[-1]*(len(his.shape)-1)) for his in model.initial_states()['histories']]\n",
    "output, next_histories = model.forward(inputs['input_ids'][0], 1, histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accbb11c-f59c-4cef-9330-d0fbc4c84014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(model.limas)):\n",
    "#     lima_shape = model.limas[i].lima_shape\n",
    "#     print(lima_shape)\n",
    "#     print(f'{i}: {lima_shape.min()}, {lima_shape.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcf9539e-9188-412d-ab27-e3b763375132",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Absolute_bert_for_masked_LM' object has no attribute 'predictor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241m.\u001b[39mrotary_denom\n",
      "File \u001b[0;32m~/miniconda3/envs/lima/lib/python3.12/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Absolute_bert_for_masked_LM' object has no attribute 'predictor'"
     ]
    }
   ],
   "source": [
    "model.predictor.rotary_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdecec59-c1d6-4131-b37e-e78e0b3bdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write first 10 questions and top 10 answer to file\n",
    "\n",
    "# samples = list(results.items())[:10]\n",
    "# for q_num, score_dict in samples:\n",
    "#     with open(f'question_{q_num}.txt', 'w') as f:\n",
    "#         f.write(f'{queries[q_num]}\\n\\n')\n",
    "#         tokens = tokenizer.convert_ids_to_tokens(tokenizer(queries[q_num], add_special_tokens=False)['input_ids'])\n",
    "#         f.write(f'{tokens}\\n\\n')\n",
    "        \n",
    "#         text_ids, text_scores = zip(*score_dict.items())\n",
    "#         text_scores = np.array(text_scores)\n",
    "#         top_10_idx = np.argsort(text_scores)[:-10:-1]\n",
    "\n",
    "#         for idx in top_10_idx:\n",
    "#             f.write(f'{corpus[text_ids[idx]]}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f43406fa-1299-46a9-8462-4398095cc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test: 看每個字往時間方向逆向轉一個 t 後，附近的字為何。理論來說會是跟這個字無關的字 (text independent)，因為這個旋轉抵銷了時間旋轉\n",
    "\n",
    "# inverse_metric_theta = - 1/model.predictor.rotary_denom**(model.predictor.dimension_nums/model.predictor.dim)\n",
    "# inverse_pos_rotation = torch.complex(inverse_metric_theta.cos(), inverse_metric_theta.sin())\n",
    "# least_effective_position_of_the_word = model.predictor.all_word_embeddings() * inverse_pos_rotation\n",
    "# least_effective_position_of_the_word = torch.concat([least_effective_position_of_the_word.real, least_effective_position_of_the_word.imag], dim=-1).detach().numpy()\n",
    "\n",
    "# least_effective_position_of_the_word.shape\n",
    "\n",
    "# %time d = pairwise_distances(word_reprs, least_effective_position_of_the_word, metric='euclidean') # metric='cosine'\n",
    "\n",
    "# %time pair = d.argsort(axis=1)[:, :10]\n",
    "\n",
    "# for input_id in tokenizer.encode(text_sample):\n",
    "#     print(f'{tokenizer.convert_ids_to_tokens(input_id)}: {tokenizer.convert_ids_to_tokens(pair[input_id])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
