{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20327,
     "status": "ok",
     "timestamp": 1712972152246,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "7qRPp5WdJF6K",
    "outputId": "a3fa7ee1-8df7-4eec-db1c-c966a9f6c823"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda:pick_a_device' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712972152246,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "th6oFB4JJXbg"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# import datasets\n",
    "# from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import utils.load_corpus as load_corpus\n",
    "import utils.losses as l\n",
    "\n",
    "import model.pick_a_model as rotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712972152247,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "HzjhZio7KkHu"
   },
   "outputs": [],
   "source": [
    "PATH = \"data\"\n",
    "folder_path = Path(f\"limanet\")\n",
    "os.makedirs(PATH/folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1712972153018,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "ORJKy4IGa9Er",
    "outputId": "f8fe9709-4c29-410e-b62e-a9a12b380b46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2293, 4743, 1012,  102,    0,    0,    0],\n",
       "        [ 101, 7632, 1010, 1045, 1005, 1049, 3960, 1012,  102]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_type = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_type)\n",
    "text = [\"I love bird.\", \"Hi, I'm Bob.\"]\n",
    "tokenized = tokenizer(text, return_tensors='pt', padding=True)\n",
    "tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 17334,
     "status": "ok",
     "timestamp": 1712972170349,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "UN6wHgSamOVz",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': tensor([ 2788,  2021,  1010,  2074,  2002,  2028,  2052,  2298,  2022,  2012,\n",
       "         13311,  1037,  2105,  7163,  1996,  2239,  2542,  2741,  2282,  2032,\n",
       "          1010,  8134,  2652,  4937,  2007, 22436,  2010,  2594, 10899,  1012,\n",
       "          1012]),\n",
       " 'batch_sizes': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]),\n",
       " 'sorted_indices': tensor([0, 1]),\n",
       " 'unsorted_indices': tensor([0, 1])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "load_corpus = importlib.reload(load_corpus)\n",
    "\n",
    "dataset = load_corpus.load_bookcorpus(tokenizer, cache_dir='~/data1-0756727/cache/huggingface')\n",
    "# dataset = load_corpus.load_msmarco(tokenizer, cache_dir='~/data1-0756727/cache/huggingface')\n",
    "\n",
    "dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1712972170349,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "PKq3J3zpmtsr",
    "outputId": "af55ae0f-ad29-4859-d3d5-0dd5bee53ac6"
   },
   "outputs": [],
   "source": [
    "# def transform(batch):\n",
    "#   tokenized = tokenizer(batch['text'], return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "#   packed = torch.nn.utils.rnn.pack_padded_sequence(tokenized['input_ids'], tokenized['attention_mask'].sum(dim=1), enforce_sorted=False, batch_first=True)\n",
    "#   to_return = {\n",
    "#     'data': packed.data,\n",
    "#     'batch_sizes': packed.batch_sizes,\n",
    "#     'attention_mask': tokenized['attention_mask'],\n",
    "#     'sorted_indices': packed.sorted_indices,\n",
    "#     'unsorted_indices': packed.unsorted_indices\n",
    "#   }\n",
    "#   return to_return\n",
    "\n",
    "# dataset.set_transform(transform)\n",
    "\n",
    "# dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1712972170350,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "OksOAPJwUgdk"
   },
   "outputs": [],
   "source": [
    "# # test\n",
    "# rotator_depth = 3\n",
    "# import importlib\n",
    "# rotator = importlib.reload(rotator)\n",
    "# model = rotator.Rotator(tokenizer.vocab_size, depth=rotator_depth, dim=128, num_heads=32, hippo_dim=16, num_hippo_heads=8)#.to(device)\n",
    "# model.eval()\n",
    "# batch = dataset['train'][:3]\n",
    "\n",
    "# gen = model(**batch)\n",
    "# predicts, targets = [torch.concat(res) for res in zip(*gen)]\n",
    "# predicts.shape, targets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48777,
     "status": "ok",
     "timestamp": 1712972219114,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "bsn-Qpozgiev",
    "outputId": "404d0576-287a-452a-fe3b-dabdc94a2fb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': torch.Size([7722]),\n",
       " 'batch_sizes': torch.Size([64]),\n",
       " 'attention_mask': torch.Size([512, 64]),\n",
       " 'sorted_indices': torch.Size([512]),\n",
       " 'unsorted_indices': torch.Size([512])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "shuffle = False\n",
    "\n",
    "class Trainloader:\n",
    "  def __init__(self, dataset, batch_size):\n",
    "    self.data = dataset\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) // batch_size + 1\n",
    "\n",
    "  def __iter__(self):\n",
    "    current_start = 0\n",
    "    for i in range(self.__len__()):\n",
    "      yield self.data[current_start:current_start+self.batch_size]\n",
    "      current_start += self.batch_size\n",
    "\n",
    "# train_dataloader = DataLoader(dataset[\"train\"], batch_size=batch_size, collate_fn=collate)\n",
    "train_loader = Trainloader((dataset.shuffle() if shuffle else dataset)['train'], batch_size=batch_size)\n",
    "\n",
    "\n",
    "n = next(iter(train_loader))\n",
    "# print(tokenizer.decode(n['data']))\n",
    "{key: n[key].shape for key in n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1712973346182,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "5nDVcTzpiDP9",
    "outputId": "f0885cd6-f532-45b4-b27e-1083216dcde7"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "learning_rate = 0.0001 # for euclidean\n",
    "# learning_rate = 0.01 # for cosine\n",
    "model_params = {\n",
    "  'rotator_depth': 3,\n",
    "  'num_heads': 32,\n",
    "  'dim': 128,\n",
    "  'hidden_dim': 32,\n",
    "  'rotary_denom': 0.5,\n",
    "  \n",
    "}\n",
    "loss_params = {\n",
    "  'sampling_word_size': 10,\n",
    "  'margin': 8, # for euclidean\n",
    "  # 'margin': .3, # for cosine\n",
    "}\n",
    "\n",
    "rotator = importlib.reload(rotator)\n",
    "l = importlib.reload(l)\n",
    "\n",
    "model = rotator.Rotator(tokenizer.vocab_size, **model_params).to(device)\n",
    "# using_loss = l.Complex_mse()\n",
    "# using_loss = l.Complex_triplet_loss(model)\n",
    "# using_loss = l.Complex_mse_triplet_loss(model)\n",
    "# using_loss = l.Complex_mse_squared_triplet_loss(model)\n",
    "using_loss = l.Mse_multiplet_loss(model, **loss_params)\n",
    "# using_loss = l.Cos_multiplet_loss(model, **loss_params)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing model output\n",
    "# batch = next(iter(train_loader))\n",
    "# batch = {key: batch[key].to(device) for key in batch}\n",
    "\n",
    "# gen = model(**batch)\n",
    "# predicts, targets = [torch.concat(res) for res in zip(*gen)]\n",
    "# losses = using_loss(predicts, targets)\n",
    "# loss = sum(losses.values())\n",
    "# loss_dict = {\n",
    "#   'total': f\"{loss.item(): .6f}\",\n",
    "#   **{key: f\"{losses[key].item(): .6f}\" for key in losses},\n",
    "# }\n",
    "\n",
    "# print(loss_dict)\n",
    "# # del batch, loss\n",
    "# # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1712973348182,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "v5owcJeinrVP"
   },
   "outputs": [],
   "source": [
    "# def backward_hook(module, gin, gout):\n",
    "#   print(f\"{len(gin)=}, {len(gout)=}\")\n",
    "#   print(*[f\"{i=}, {gi.shape=}, {gi.mean()=}, {gi.min()=}, {gi.max()=}\" for i, gi in enumerate(gin)], sep='\\n')\n",
    "#   print(*[f\"{i=}, {go.shape=}, {go.mean()=}, {go.min()=}, {go.max()=}\" for i, go in enumerate(gout)], sep='\\n')\n",
    "\n",
    "# model.limas[0].register_backward_hook(backward_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1712972219114,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "fjwk_J2biBnE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20240504.18:41:07'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_string = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=8))).strftime('%Y%m%d.%H:%M:%S')\n",
    "subfolder_path = Path(time_string)\n",
    "\n",
    "os.makedirs(PATH/folder_path/subfolder_path, exist_ok=True)\n",
    "\n",
    "with open(PATH/folder_path/subfolder_path/f'parameters.json', 'w') as f:\n",
    "  f.write(json.dumps({\n",
    "    'tokenizer_type': tokenizer_type,\n",
    "    'model': str(model),\n",
    "    'model_params': model_params,\n",
    "    'learning_rate': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': shuffle,\n",
    "    'loss_type': str(using_loss),\n",
    "    'loss_params': loss_params,\n",
    "  }))  \n",
    "time_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjU3iaLjiFsN",
    "outputId": "691fe7e1-4bb0-451a-c5ad-2e5a219c135e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 185/144540 [01:52<24:18:45,  1.65it/s, total=505.241882, multiplet_loss=7.633089, mse=497.608795]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 50\u001b[0m\n\u001b[1;32m     45\u001b[0m bar\u001b[38;5;241m.\u001b[39mset_postfix(loss_dict)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# with torch.autograd.detect_anomaly(True):\u001b[39;00m\n\u001b[1;32m     48\u001b[0m   \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[1;32m     49\u001b[0m   \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=10, norm_type=2)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# break \u001b[39;00m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/lima/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lima/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "save_every_n_batches = 5000\n",
    "\n",
    "\n",
    "# os.makedirs(PATH/folder_path/subfolder_path, exist_ok=True)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch_num, epoch in enumerate(range(num_epochs)):\n",
    "  bar = tqdm(train_loader)\n",
    "\n",
    "  for batch_num, batch in enumerate(bar):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch = {key: batch[key].to(device) for key in batch}\n",
    "    gen = model(**batch)\n",
    "\n",
    "    total_losses = defaultdict(lambda: torch.tensor([0], dtype=torch.float).to(device))\n",
    "    while True: \n",
    "      results = []\n",
    "      try:\n",
    "        for _ in range(100):\n",
    "          results.append(next(gen))\n",
    "        predicts, targets = [torch.concat(pred_or_targ) for pred_or_targ in zip(*results)]\n",
    "        losses = using_loss(predicts, targets)\n",
    "        for loss_name, value in losses.items():\n",
    "          total_losses[loss_name] += value\n",
    "          \n",
    "      except StopIteration:\n",
    "        if len(results) > 0:\n",
    "          predicts, targets = [torch.concat(pred_or_targ) for pred_or_targ in zip(*results)]\n",
    "          losses = using_loss(predicts, targets)\n",
    "          for loss_name, value in losses.items():\n",
    "            total_losses[loss_name] += value\n",
    "        break\n",
    "    \n",
    "    loss = sum(total_losses.values())\n",
    "\n",
    "    loss_dict = {\n",
    "      'total': f\"{loss.item(): .6f}\",\n",
    "      **{key: f\"{losses[key].item(): .6f}\" for key in losses},\n",
    "    }\n",
    "      \n",
    "    bar.set_postfix(loss_dict)\n",
    "\n",
    "    # with torch.autograd.detect_anomaly(True):\n",
    "      # loss.backward()\n",
    "      # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=10, norm_type=2)\n",
    "    loss.backward()\n",
    "    # break \n",
    "    optimizer.step()\n",
    "\n",
    "    if (batch_num % 100 == 0):\n",
    "      # torch.cuda.empty_cache()\n",
    "      loss_history.append([epoch_num, batch_num, loss_dict])\n",
    "      # print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, batch {batch_num}\")\n",
    "\n",
    "    if (batch_num % save_every_n_batches == 0):\n",
    "      prefix = f\"batch_{batch_num}-\"\n",
    "      torch.save(model, PATH/folder_path/subfolder_path/(prefix + f'model.pt'))\n",
    "        \n",
    "      with open(PATH/folder_path/subfolder_path/'history.json', 'w') as f:\n",
    "        f.write(json.dumps(loss_history))  \n",
    "\n",
    "      # torch.cuda.empty_cache()\n",
    "\n",
    "    if (batch_num in [100, 500, 1000, 1500, 2000, 3000, 7500, 12500, 17500, 22500]):\n",
    "        prefix = f\"batch_{batch_num}-\"\n",
    "        torch.save(model, PATH/folder_path/subfolder_path/(prefix + 'model.pt'))\n",
    "        \n",
    "        with open(PATH/folder_path/subfolder_path/'history.json', 'w') as f:\n",
    "          f.write(json.dumps(loss_history))  \n",
    "\n",
    "    # del batch, loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOYhgT3ZRZ8Mnsn8jR0fZTM",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
