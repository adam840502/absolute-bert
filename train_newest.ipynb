{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20327,
     "status": "ok",
     "timestamp": 1712972152246,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "7qRPp5WdJF6K",
    "outputId": "a3fa7ee1-8df7-4eec-db1c-c966a9f6c823"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda:pick_a_device' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712972152246,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "th6oFB4JJXbg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/lima/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import model.rotator_lima4_hippo as rotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712972152247,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "HzjhZio7KkHu"
   },
   "outputs": [],
   "source": [
    "PATH = \"data\"\n",
    "folder_path = Path(f\"limanet\")\n",
    "os.makedirs(PATH/folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1712972153018,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "ORJKy4IGa9Er",
    "outputId": "f8fe9709-4c29-410e-b62e-a9a12b380b46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2293, 4743, 1012,  102,    0,    0,    0],\n",
       "        [ 101, 7632, 1010, 1045, 1005, 1049, 3960, 1012,  102]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_type = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_type)\n",
    "text = [\"I love bird.\", \"Hi, I'm Bob.\"]\n",
    "tokenized = tokenizer(text, return_tensors='pt', padding=True)\n",
    "tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 17334,
     "status": "ok",
     "timestamp": 1712972170349,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "UN6wHgSamOVz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bookcorpus\", cache_dir='~/data1-0756727/cache/huggingface')\n",
    "# dataset.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1712972170349,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "PKq3J3zpmtsr",
    "outputId": "af55ae0f-ad29-4859-d3d5-0dd5bee53ac6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': tensor([ 2788,  2021,  1010,  2074,  2002,  2028,  2052,  2298,  2022,  2012,\n",
       "         13311,  1037,  2105,  7163,  1996,  2239,  2542,  2741,  2282,  2032,\n",
       "          1010,  8134,  2652,  4937,  2007, 22436,  2010,  2594, 10899,  1012,\n",
       "          1012]),\n",
       " 'batch_sizes': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]),\n",
       " 'sorted_indices': tensor([0, 1]),\n",
       " 'unsorted_indices': tensor([0, 1])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(batch):\n",
    "  tokenized = tokenizer(batch['text'], return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "  packed = torch.nn.utils.rnn.pack_padded_sequence(tokenized['input_ids'], tokenized['attention_mask'].sum(dim=1), enforce_sorted=False, batch_first=True)\n",
    "  to_return = {\n",
    "    'data': packed.data,\n",
    "    'batch_sizes': packed.batch_sizes,\n",
    "    'attention_mask': tokenized['attention_mask'],\n",
    "    'sorted_indices': packed.sorted_indices,\n",
    "    'unsorted_indices': packed.unsorted_indices\n",
    "  }\n",
    "  return to_return\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1712972170350,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "OksOAPJwUgdk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([42, 64]), torch.Size([42, 64]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # test\n",
    "# rotator_depth = 3\n",
    "# import importlib\n",
    "# rotator = importlib.reload(rotator)\n",
    "# model = rotator.Rotator(tokenizer.vocab_size, depth=rotator_depth)#.to(device)\n",
    "# model.eval()\n",
    "# batch = dataset['train'][:3]\n",
    "\n",
    "# gen = model(**batch)\n",
    "# predicts, targets = [torch.concat(res) for res in zip(*gen)]\n",
    "# predicts.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48777,
     "status": "ok",
     "timestamp": 1712972219114,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "bsn-Qpozgiev",
    "outputId": "404d0576-287a-452a-fe3b-dabdc94a2fb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': torch.Size([7111]),\n",
       " 'batch_sizes': torch.Size([56]),\n",
       " 'attention_mask': torch.Size([512, 56]),\n",
       " 'sorted_indices': torch.Size([512]),\n",
       " 'unsorted_indices': torch.Size([512])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "shuffle = False\n",
    "\n",
    "class Trainloader:\n",
    "  def __init__(self, dataset, batch_size):\n",
    "    self.data = dataset\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) // batch_size + 1\n",
    "\n",
    "  def __iter__(self):\n",
    "    current_start = 0\n",
    "    for i in range(self.__len__()):\n",
    "      current_start += self.batch_size\n",
    "      yield self.data[current_start:current_start+self.batch_size]\n",
    "\n",
    "# train_dataloader = DataLoader(dataset[\"train\"], batch_size=batch_size, collate_fn=collate)\n",
    "train_loader = Trainloader((dataset.shuffle() if shuffle else dataset)['train'], batch_size=batch_size)\n",
    "\n",
    "\n",
    "n = next(iter(train_loader))\n",
    "# print(tokenizer.decode(n['data']))\n",
    "{key: n[key].shape for key in n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1712973346182,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "5nDVcTzpiDP9",
    "outputId": "f0885cd6-f532-45b4-b27e-1083216dcde7"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "learning_rate = 0.0001\n",
    "model_params = {\n",
    "  'rotator_depth': 3,\n",
    "  'num_heads': 32,\n",
    "  'dim': 128,\n",
    "  'hidden_dim': 32,\n",
    "  'rotary_denom': 0.5,\n",
    "  'margin': 8,\n",
    "}\n",
    "loss_params = {\n",
    "  'sampling_word_size': 10\n",
    "}\n",
    "\n",
    "class Complex_mse(nn.Module):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "\n",
    "  def __repr__(self):\n",
    "      return 'mse'\n",
    "\n",
    "  def forward(self, predicts, targets):\n",
    "    diffs = (predicts - targets)\n",
    "    # print(diffs)\n",
    "    # diff_square_norms = diffs * diffs.conj()\n",
    "    diff_square_norms = diffs.real**2 + diffs.imag**2\n",
    "    # diff_square_norms = torch.clamp(diff_square_norms, min=-.5, max=.5)\n",
    "    # print(diff_square_norms)\n",
    "    return {\n",
    "        'mse': diff_square_norms.real.sum(dim=[1]).mean()\n",
    "    }\n",
    "\n",
    "class Complex_mse_with_inverse_norm(Complex_mse):\n",
    "    def __init__(self, relax=.25, min_dist=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.relax = relax\n",
    "        self.min_dist = min_dist\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'mse_with_inverse_norm(relax={self.relax}, min_dist={self.min_dist})'\n",
    "\n",
    "    def forward(self, predicts, targets):\n",
    "        inverse_norm = torch.clamp(1/(predicts.norm(dim=-1)+self.relax) - 1/(self.min_dist+self.relax), min=0)\n",
    "        return  {\n",
    "            **super().forward(predicts, targets),\n",
    "            'inverse_norm': inverse_norm.mean()\n",
    "        }\n",
    "\n",
    "import numpy as np\n",
    "class Complex_triplet_loss(nn.Module):\n",
    "  def __init__(self, model, sampling_word_size=10, margin=5, distance_metric='l2', **kwargs):\n",
    "    super().__init__()\n",
    "    self.sampling_word_size = sampling_word_size\n",
    "    self.distance_metric = distance_metric\n",
    "    self.margin = margin\n",
    "    self.model = model\n",
    "\n",
    "    if distance_metric != 'l2':\n",
    "      raise NotImplementedError\n",
    "\n",
    "  def __repr__(self):\n",
    "      return f'triplet_loss(margin={self.margin})'\n",
    "\n",
    "  def forward(self, predicts, targets):\n",
    "    sampled_word_vecs = self.model.predictor.all_word_embeddings()[np.random.choice(self.model.vocab_size, size=self.sampling_word_size)]\n",
    "\n",
    "    pos_dist = (predicts - targets).norm(dim=1)\n",
    "    sampled_dists = self.model.pairwise_distance(targets, sampled_word_vecs, distance=self.distance_metric)\n",
    "    neg_dist = sampled_dists.min(dim=1).values\n",
    "      \n",
    "    return {\n",
    "        'triplet_loss': torch.clamp(pos_dist-neg_dist+self.margin, min=0).mean()\n",
    "    }\n",
    "\n",
    "class Multiplet_loss(nn.Module):\n",
    "  def __init__(self, model, sampling_word_size=10, margin=5, distance_metric='l2', **kwargs):\n",
    "    super().__init__()\n",
    "    self.sampling_word_size = sampling_word_size\n",
    "    self.distance_metric = distance_metric\n",
    "    self.margin = margin\n",
    "    self.model = model\n",
    "\n",
    "    if distance_metric != 'l2':\n",
    "      raise NotImplementedError\n",
    "\n",
    "  def __repr__(self):\n",
    "      return f'multiplet_loss(margin={self.margin}, sampling_word_size={self.sampling_word_size})'\n",
    "\n",
    "  def forward(self, predicts, targets):\n",
    "    sampled_word_vecs = self.model.predictor.all_word_embeddings()[np.random.choice(self.model.vocab_size, size=self.sampling_word_size)]\n",
    "\n",
    "    pos_dist = (predicts - targets).norm(dim=1) #shape: [batch]\n",
    "    sampled_dists = self.model.pairwise_distance(targets, sampled_word_vecs, distance=self.distance_metric) \n",
    "    neg_dist = sampled_dists.min(dim=1).values\n",
    "    triplets = (pos_dist[:, None] - neg_dist + self.margin) # shape: [batch, self.sampling_word_size]\n",
    "    \n",
    "    return {\n",
    "        'multiplet_loss': torch.clamp(triplets, min=0).mean()\n",
    "    }\n",
    "\n",
    "        \n",
    "class Complex_mse_triplet_loss(nn.Module):\n",
    "  def __init__(self, model, sampling_word_size=10, margin=5, distance_metric='l2', **kwargs):\n",
    "    super().__init__()\n",
    "    self.triplet = Complex_triplet_loss(model, sampling_word_size, margin, distance_metric)\n",
    "    self.mse = Complex_mse()\n",
    "\n",
    "  def __repr__(self):\n",
    "      return f'{str(self.triplet)}+{str(self.mse)}'\n",
    "\n",
    "  def forward(self, predicts, targets):\n",
    "    return {\n",
    "      **self.triplet(predicts, targets),\n",
    "      **self.mse(predicts, targets)\n",
    "    }\n",
    "\n",
    "class Complex_mse_squared_triplet_loss(nn.Module):\n",
    "  def __init__(self, model, sampling_word_size=10, margin=5, distance_metric='l2', **kwargs):\n",
    "    super().__init__()\n",
    "    self.triplet = Complex_triplet_loss(model, sampling_word_size, margin, distance_metric)\n",
    "    self.mse = Complex_mse()\n",
    "\n",
    "  def __repr__(self):\n",
    "      return f'sq{str(self.triplet)}+{str(self.mse)}'\n",
    "\n",
    "  def forward(self, predicts, targets):\n",
    "    return {\n",
    "      'sqtriplet_loss': self.triplet(predicts, targets)['triplet_loss']**2,\n",
    "      **self.mse(predicts, targets)\n",
    "    }\n",
    "\n",
    "class Mse_multiplet_loss(nn.Module):\n",
    "  def __init__(self, model, sampling_word_size=10, margin=5, distance_metric='l2', **kwargs):\n",
    "    super().__init__()\n",
    "    self.multiplet = Multiplet_loss(model, sampling_word_size, margin, distance_metric)\n",
    "    self.mse = Complex_mse()\n",
    "\n",
    "  def __repr__(self):\n",
    "      return f'{str(self.multiplet)}+{str(self.mse)}'\n",
    "\n",
    "  def forward(self, predicts, targets):\n",
    "    return {\n",
    "      **self.multiplet(predicts, targets),\n",
    "      **self.mse(predicts, targets)\n",
    "    }\n",
    "\n",
    "\n",
    "rotator = importlib.reload(rotator)\n",
    "model = rotator.Rotator(tokenizer.vocab_size, **model_params).to(device)\n",
    "# using_loss = Complex_mse()\n",
    "# using_loss = Complex_triplet_loss(model)\n",
    "# using_loss = Complex_mse_triplet_loss(model)\n",
    "# using_loss = Complex_mse_squared_triplet_loss(model)\n",
    "using_loss = Mse_multiplet_loss(model, **loss_params)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # testing model output\n",
    "# batch = next(iter(train_loader))\n",
    "# batch = {key: batch[key].to(device) for key in batch}\n",
    "\n",
    "# gen = model(**batch)\n",
    "# predicts, targets = [torch.concat(res) for res in zip(*gen)]\n",
    "# losses = using_loss(predicts, targets)\n",
    "# loss = sum(losses.values())\n",
    "\n",
    "# loss_dict = {\n",
    "#   'total': f\"{loss.item(): .6f}\",\n",
    "#   **{key: f\"{losses[key].item(): .6f}\" for key in losses},\n",
    "# }\n",
    "# print(loss_dict)\n",
    "\n",
    "# del batch, loss\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1712973348182,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "v5owcJeinrVP",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def backward_hook(module, gin, gout):\n",
    "#   print(f\"{len(gin)=}, {len(gout)=}\")\n",
    "#   print(*[f\"{i=}, {gi.shape=}, {gi.mean()=}, {gi.min()=}, {gi.max()=}\" for i, gi in enumerate(gin)], sep='\\n')\n",
    "#   print(*[f\"{i=}, {go.shape=}, {go.mean()=}, {go.min()=}, {go.max()=}\" for i, go in enumerate(gout)], sep='\\n')\n",
    "\n",
    "# model.limas[0].register_backward_hook(backward_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1712972219114,
     "user": {
      "displayName": "溫子漢",
      "userId": "03827734041803749612"
     },
     "user_tz": -480
    },
    "id": "fjwk_J2biBnE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20240427.15:29:39'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_string = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=8))).strftime('%Y%m%d.%H:%M:%S')\n",
    "subfolder_path = Path(f\"{time_string}-batch_size_{batch_size}\")\n",
    "\n",
    "os.makedirs(PATH/folder_path/subfolder_path, exist_ok=True)\n",
    "\n",
    "with open(PATH/folder_path/subfolder_path/f'parameters.json', 'w') as f:\n",
    "  f.write(json.dumps({\n",
    "    'tokenizer_type': tokenizer_type,\n",
    "    'model': str(model),\n",
    "    **model_params,\n",
    "    'learning_rate': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': shuffle,\n",
    "    'loss_type': str(using_loss),\n",
    "    **loss_params,\n",
    "  }))  \n",
    "time_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjU3iaLjiFsN",
    "outputId": "691fe7e1-4bb0-451a-c5ad-2e5a219c135e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███                       | 16789/144540 [1:41:19<10:30:41,  3.38it/s, total=73.347336, multiplet_loss=1.492947, mse=71.854385]Token indices sequence length is longer than the specified maximum sequence length for this model (1119 > 512). Running this sequence through the model will result in indexing errors\n",
      " 19%|████▉                     | 27701/144540 [2:53:31<12:11:55,  2.66it/s, total=81.986778, multiplet_loss=7.776369, mse=74.210411]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.82 GiB. GPU 5 has a total capacity of 15.78 GiB of which 881.75 MiB is free. Process 6126 has 14.92 GiB memory in use. Of the allocated memory 10.18 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m bar\u001b[38;5;241m.\u001b[39mset_postfix(loss_dict)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# with torch.autograd.detect_anomaly(True):\u001b[39;00m\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=10, norm_type=2)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# break \u001b[39;00m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/lima/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lima/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.82 GiB. GPU 5 has a total capacity of 15.78 GiB of which 881.75 MiB is free. Process 6126 has 14.92 GiB memory in use. Of the allocated memory 10.18 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "save_every_n_batches = 5000\n",
    "\n",
    "\n",
    "# os.makedirs(PATH/folder_path/subfolder_path, exist_ok=True)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch_num, epoch in enumerate(range(num_epochs)):\n",
    "  bar = tqdm(train_loader)\n",
    "\n",
    "  for batch_num, batch in enumerate(bar):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch = {key: batch[key].to(device) for key in batch}\n",
    "\n",
    "    gen = model(**batch)\n",
    "    predicts, targets = [torch.concat(res) for res in zip(*gen)]\n",
    "    losses = using_loss(predicts, targets)\n",
    "    loss = sum(losses.values())\n",
    "\n",
    "    loss_dict = {\n",
    "      'total': f\"{loss.item(): .6f}\",\n",
    "      **{key: f\"{losses[key].item(): .6f}\" for key in losses},\n",
    "    }\n",
    "      \n",
    "    bar.set_postfix(loss_dict)\n",
    "\n",
    "    # with torch.autograd.detect_anomaly(True):\n",
    "      # loss.backward()\n",
    "      # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=10, norm_type=2)\n",
    "    loss.backward()\n",
    "    # break \n",
    "    optimizer.step()\n",
    "\n",
    "    if (batch_num % 100 == 0):\n",
    "      torch.cuda.empty_cache()\n",
    "      loss_history.append([epoch_num, batch_num, loss_dict])\n",
    "      # print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, batch {batch_num}\")\n",
    "\n",
    "    if (batch_num % save_every_n_batches == 0):\n",
    "      prefix = f\"batch_{batch_num}-\"\n",
    "      torch.save(model, PATH/folder_path/subfolder_path/(prefix + f'model.pt'))\n",
    "        \n",
    "      with open(PATH/folder_path/subfolder_path/'history.json', 'w') as f:\n",
    "        f.write(json.dumps(loss_history))  \n",
    "\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "    if (batch_num in [100, 500, 1000, 1500, 2000, 3000, 7500, 12500, 17500, 22500]):\n",
    "        prefix = f\"batch_{batch_num}-\"\n",
    "        torch.save(model, PATH/folder_path/subfolder_path/(prefix + f'model.pt'))\n",
    "        \n",
    "        with open(PATH/folder_path/subfolder_path/'history.json', 'w') as f:\n",
    "          f.write(json.dumps(loss_history))  \n",
    "\n",
    "    del batch, loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOYhgT3ZRZ8Mnsn8jR0fZTM",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
